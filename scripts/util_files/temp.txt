import configparser
import sched
import time
import datetime
import sys

from class_files.postgres_class import *
from util_files.toiHeadlinesScraper import *
from util_files.utilFunctions import *

from logger_config import get_logger

logger = get_logger('scrape')

class scrapeFromWeb():

    def __init__(self):
        config = configparser.ConfigParser(allow_no_value=True)
        config.read("config.ini")
        self.inputType = config["web_scraper"]["input_type"]
        self.url=config["web_scraper"]["url"]
        self.kafkaTopicName = config["web_scraper"]["kafka_topic_name"]
        self.kafkaBootstrapServer = config["web_scraper"]["kafka_bootstrap_server"]
        self.intervalTime = config["web_scraper"]["scraping_interval"] #in mins
        self.scheduler = sched.scheduler(time.time, time.sleep)
        self.processedFolderPath = config["web_scraper"]["processed_folder_path"]
        self.errorFolderPath = config["web_scraper"]["error_folder_path"]

         #Params to add to Database
        self.sourceId=config["web_scraper"]["source_connector_id"]
        self.ingestionType="Stream"
        self.ingestionFormat="HTML"

        self.dbConnect = postgresDb()

    def runScraper(self):
        currentTimeStamp = str(int(datetime.datetime.now().timestamp()))
        systemFilename = currentTimeStamp +"__TOIWebScrapingFromUrl.html"
        
        pageResponse = None
        try:
            newsData, pageResponse = timesOfIndia(self.inputType, self.url)
        except:
            print("Error Scraping data from web")
            logger.error("Error Scraping data from web")
            return
        
        if(newsData != {}):
            print("Data scraped from web Successfully")
            logger.info("Data scraped from web Successfully")
            try:
                kafkaHeaders = [
                    ("FileName", systemFilename.encode('utf-8')),
                    ("IngestionTimestamp", currentTimeStamp.encode('utf-8'))
                ]
                pushDataToKafkaTopic(self.kafkaTopicName, newsData, self.kafkaBootstrapServer, kafkaHeaders)
                print("Data pushed to Kafka Topic successfully")
                logger.info("Data pushed to Kafka Topic successfully")
            except:
                print("Unable to push data Kafka Topic")
                logger.error("Unable to push data Kafka Topic")
                destinationFilePath = os.path.join(os.path.dirname(self.processedFolderPath), systemFilename)
                dumpHtmlToFile(pageResponse, destinationFilePath)
                print("Html data dumped to file at error location due to failure")
                logger.error("Html data dumped to file at error location due to failure")
                dataToInsert = {
                    "original_filename": None,
                    "system_filename": systemFilename,
                    "ingestion_source": self.sourceId,
                    "ingestion_type": self.ingestionType,
                    "ingestion_format": self.ingestionFormat,
                    "checksum": getFileChecksum(destinationFilePath),
                    "csv_file_format": None,
                    "status": "Failure", 
                    "failure_reason": "Unable to push data Kafka Topic",
                    "created_on": int(datetime.datetime.now().timestamp())
                }
                try:
                    self.dbConnect.insertIntoDb(dataToInsert=dataToInsert)
                    print("Data Logged to Database Successfully")
                    logger.info("Data Logged to Database Successfully")
                except:
                    print("Unable to Log to Database!!")
                    logger.error("Unable to Log to Database!!")
                    return
        else:
            print("Unable to scrape Data!! Wrong url provided!! Pls check the url and try Again!!")
            logger.error("Unable to scrape Data, Wrong url provided")
            return
        
        destinationFilePath = os.path.join(os.path.dirname(self.processedFolderPath), systemFilename)
        dumpHtmlToFile(pageResponse, destinationFilePath)
        print("Html data dumped to file at processed location successfully")
        logger.info("Html data dumped to file at processed location successfully")
        dataToInsert = {
            "original_filename": None,
            "system_filename": systemFilename,
            "ingestion_source": self.sourceId,
            "ingestion_type": self.ingestionType,
            "ingestion_format": self.ingestionFormat,
            "checksum": getFileChecksum(destinationFilePath),
            "csv_file_format": None,
            "status": "Success", 
            "failure_reason": None,
            "created_on": int(datetime.datetime.now().timestamp())
        }

        try:
            self.dbConnect.insertIntoDb(dataToInsert=dataToInsert)
            print("Data Logged to Database Successfully")
            logger.info("Data Logged to Database Successfully")
        except:
            print("Unable to Log to Database!!")
            logger.error("Unable to Log to Database!!")
            return
        
        return


    def schedule_next_event(self):
        self.scheduler.enter(int(float(self.intervalTime)*60), 1, self.schedule_next_event)
        print(f"Scheduled Next Web Scraping Process to run after {self.intervalTime} Minutes")
        self.runScraper()

    def start(self):
        # self.scheduler.enter(0, 1, self.schedule_next_event)
        print("Scheduled First Web Scraping Process")
        self.runScraper()  # Run the scraper immediately once
        self.schedule_next_event()
        self.scheduler.run()
        
    def stop(self):
        for event in self.scheduler.queue:
            self.scheduler.cancel(event)
        print("Stopped Web Scraping Process")
        sys.exit(0)

import psycopg2
import configparser

class postgresDb:
    
    def __init__(self):
        config = configparser.ConfigParser(allow_no_value=True)
        config.read("config.ini")
        
        self.dbName = config.get("database", "name")
        self.dbUser = config.get("database", "user")
        self.dbPassword = config.get("database", "password")
        self.dbHost = config.get("database", "host")
        self.dbPort = config.get("database", "port")
        self.tableName = config.get("database", "table_name")

        self.insertValuesHeaders = ["original_filename", #Original File Name incase it is batch/file ingestion
                                    "system_filename",#System Generated File Name
                                    "ingestion_source",#What is the source - id or name of the source connector
                                    "ingestion_type", #Batch or Stream - Bacth refers to file ingestion and Stream refers to direct from web or socket
                                    "ingestion_format",# Which data format - PCAP, CDR, WebScraping etc.....,
                                    "checksum", # Checksum of the file
                                    "csv_file_format", # If it is a csv, then what is the format of file identified
                                    "status", # Success, Failure
                                    "failure_reason", #Failure Reason
                                    "created_on" #CreatedOn Timestamp
                                    ]

    def connectToDb(self):
        self.connection = psycopg2.connect(dbname=self.dbName, 
                                      user=self.dbUser, 
                                      password=self.dbPassword, 
                                      host=self.dbHost, 
                                      port=self.dbPort)
        
        return
    
    def closeDbConnection(self):
        
        self.connection.close()
        
        return

    '''
    The dataToInsert must be a json consisting of following fields
    {
        original_filename, - Original File Name incase it is batch/file ingestion
        system_filename, - System Generated File Name
        ingestion_source, - What is the source - id or name of the source connector
        ingestion_type, - Batch or Stream - Bacth refers to file ingestion and Stream refers to direct from web or socket
        ingestion_format, - Which data format - PCAP, CDR, WebScraping etc.....,
        checksum, - Checksum of the file
        csv_file_format, - If it is a csv, then what is the format of file identified
        status, - Success, Failure
        failure_reason,
        created_on
    }
    '''
    def insertIntoDb(self, dataToInsert):

        insertValuesList = []
        for index in range(0, len(self.insertValuesHeaders)):
            insertValuesList.append(dataToInsert[self.insertValuesHeaders[index]])

        print(tuple(insertValuesList))

        try:
            self.connectToDb()
            cursor = self.connection.cursor()

            insertQuery = """
                INSERT INTO """+str(self.tableName)+ """ (
                    original_filename,
                    system_filename,
                    ingestion_source,
                    ingestion_type,
                    ingestion_format,
                    checksum,
                    csv_file_format,
                    status,
                    failure_reason,
                    created_on
                )
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, to_timestamp(%s))
            """

            print(insertQuery)
            cursor.execute(insertQuery, tuple(insertValuesList))
            self.connection.commit()
            cursor.close()
        
        except psycopg2.DataError as e:
            print(f"Ignoring database insertion error: {str(e)}")

        except Exception as e:
            print(f"Error inserting into database: {str(e)}")

        finally:
            self.closeDbConnection()

import os
import hashlib
import datetime
from kafka import KafkaProducer
import json

from class_files.postgres_class import *

from logger_config import get_logger
logger = get_logger('util')

#Function to get checksum of file
def getFileChecksum(filePath):
    sha256Hash = hashlib.sha256()
    with open(filePath, "rb") as file:
        for byteBlock in iter(lambda: file.read(4096), b""):
            sha256Hash.update(byteBlock)
    return sha256Hash.hexdigest()


def moveFile(sourcefilePath, destinationFilePath):
    if os.path.exists(sourcefilePath):
        os.rename(sourcefilePath, destinationFilePath)
        return True
    else:
        print(f"Error: File not found at {sourcefilePath}")
        logger.error(f"Error: File not found at {sourcefilePath}")
        return False


def handle_error(sourcefilePath, errorMessage, errorDirPath, sysFileName, ingestionSourceId, ingestionType, ingestionFormat):
    print(f"Error processing file: {sourcefilePath}")
    logger.error(f"Error processing file: {sourcefilePath}")
    destinationFilePath = os.path.join(os.path.dirname(errorDirPath), sysFileName)
    print(f"Error file renamed to: {destinationFilePath}")
    logger.error(f"Error file renamed to: {destinationFilePath}")

    if(os.path.exists(sourcefilePath)):
        moveStat = moveFile(sourcefilePath, destinationFilePath)
        if(moveStat):
            print("Moved to Error Folder")
            logger.info("Moved to Error Folder")
        else:
            print("Unable to move due to an error!!")
            logger.error("Unable to move due to an error!!")
            return
    else:
        print("Source file not present in the given path")
        logger.info("Source file not present in the given path")
        return
    
    dataToInsert = {
            "original_filename": sourcefilePath.split("/")[-1],
            "system_filename": sysFileName,
            "ingestion_source": ingestionSourceId,
            "ingestion_type": ingestionType,
            "ingestion_format": ingestionFormat,
            "checksum": getFileChecksum(destinationFilePath),
            "csv_file_format": None,
            "status": "Failure", 
            "failure_reason": errorMessage,
            "created_on": int(datetime.datetime.now().timestamp())
        }

    try:
        dbConnect = postgresDb()
        dbConnect.insertIntoDb(dataToInsert=dataToInsert)
        print("Data Logged to Database Successfully")
        logger.info("Data Logged to Database Successfully")
    except:
        print("Unable to Log to Database!!")
        logger.error("Unable to Log to Database!!")
        return

#Function to Push Data to KAFKA Topic, Data must be a JSON(dict) of Key,Value Pairs
def pushDataToKafkaTopic(topicName, data, bootstrap_server, headers):
    #Create Kafka Producer Instance
    producer = KafkaProducer(bootstrap_servers = bootstrap_server)

    for dataKey in data:
        # print(topicName, key, json.dumps(data[key]))
        # producer.send(topicName, json.dumps({"hashKey":key}), json.dumps(data[key]))
        producer.send(topicName, key=dataKey.encode('utf-8'), value=json.dumps(data[dataKey]).encode('utf-8'), headers=headers)

import requests
from bs4 import BeautifulSoup
import json

from logger_config import get_logger
logger = get_logger('scraper')

#Function to generate hash key for a given text
def getHashValue(textdata):
    hashVal = hash(textdata)
    if(hashVal < 0):
        hashVal = hashVal*-1
    
    return hashVal

#Function to generate JSON for each newsline
def createNewsJson(categories, newsLine, url, content, images):
    data = {
        "categories": categories,
        "news": newsLine,
        "url": url,
        "content": content,
        "images": images  # List of image URLs
    }
    return data

def scrapeArticleContent(url):
    try:
        response = requests.get(url)
        articleSoup = BeautifulSoup(response.content, "html.parser")
        
        # Find content div and image div based on ToI website's structure
        contentDiv = articleSoup.find('div', {'class': '_s30J'})
        imgDiv = articleSoup.find('div', {'class': 'T22zO'})
        
        # Initialize variables to store content and images
        content = ""
        images = []
        
        # Extract text content
        if contentDiv:
            content = contentDiv.get_text(separator='\n', strip=True)
        else:
            content = "Content not available"
        
        # Extract image URLs
        if imgDiv:
            img_tags = imgDiv.find_all('img')
            for img_tag in img_tags:
                img_src = img_tag.get('src')
                if img_src:
                    images.append(img_src)
        
        return content, images
    
    except Exception as e:
        print(f"Failed to scrape content for URL {url}: {e}")
        logger.error(f"Failed to scrape content for URL {url}: {e}")
        return "Failed to retrieve content"

def scrapeToiHeadlines(soup):
    toiNews = {}
    base_url = "https://timesofindia.indiatimes.com"
    #News Headlines Scraping
    #Scraping the Top Div
    for mainDiv in soup.find_all('div', {'id': 'c_0201'}):
        for innerDiv in mainDiv:
            #Scrape News Headlines Section
            if(innerDiv['id'] == "c_headlines_wdt_1"):
                mainCategory = innerDiv.h1.text
                #Scraping Top Pane of News Headlines
                categories = mainCategory+","+"Top Pane Of News Headlines"
                print(categories)
                for innerDivItem in innerDiv.find_all('div', {'class':'top-newslist'}):
                    for listItem in innerDivItem.find_all('li'):
                        headline = listItem.text.strip()
                        link = listItem.find('a')['href']
                        full_url = base_url + link
                        content, images = scrapeArticleContent(full_url)
                        headlineDict = createNewsJson(categories, headline, full_url, content, images)
                        hashKey = str(getHashValue(headlineDict["news"]))
                        toiNews[hashKey] = headlineDict
                #Scraping Bottom Pane of News Headlines
                categories = mainCategory+","+"Bottom Pane Of News Headlines"
                for innerDivItem in innerDiv.find_all('div', {'class':'headlines-list'}):
                    for listitem in innerDivItem.find_all('li'):
                        headline = listitem.text.strip()
                        link = listitem.find('a')['href']
                        full_url = base_url + link
                        content, images = scrapeArticleContent(full_url)
                        headlineDict = createNewsJson(categories, headline, full_url, content, images)
                        hashKey = str(getHashValue(headlineDict["news"]))
                        toiNews[hashKey] = headlineDict
            #Scrape MetroCities Section
            if(innerDiv['id'] == "c_020101"):
                mainCategory=innerDiv.h2.text
                mcDiv = innerDiv.find_all('div', {"id":"c_headlines_wdt_1"})[0]
                for innerDivItem in mcDiv.find_all(['h2', 'div'], {"class":["heading2","top-newslist", "headlines-list"]}):
                    #Scrape the Cities Name
                    if("heading2" in ' '.join(innerDivItem['class'])):
                        categories = mainCategory+","+innerDivItem.a.text
                    #Scrape all the news for the given cities
                    elif("top-newslist" in ' '.join(innerDivItem['class']) or "headlines-list" in ' '.join(innerDivItem['class'])):
                        for listitem in innerDivItem.find_all('li'):
                            headline = listitem.text.strip()
                            link = listitem.find('a')['href']
                            full_url = base_url + link
                            content, images = scrapeArticleContent(full_url)
                            headlineDict = createNewsJson(categories, headline, full_url, content, images)
                            hashKey = str(getHashValue(headlineDict["news"]))
                            toiNews[hashKey] = headlineDict
                #Scraping Business News Section
                bnDiv = innerDiv.find_all('div', {"id":"c_headlines_wdt_2", "class":"business"})[0]
                for innerDivItem in bnDiv:
                    #Scraping News Heading
                    if("heading1" in " ".join(innerDivItem['class'])):
                        mainCategory = innerDivItem.a.text
                    #Scraping SubHeading
                    if("business" in " ".join(innerDivItem['class'])):
                        categories = mainCategory + "," + innerDivItem.h4.text
                    #Scraping News Lines
                        for listitem in innerDivItem.find_all('li'):
                            headline = listitem.text.strip()
                            link = listitem.find('a')['href']
                            full_url = base_url + link
                            content, images = scrapeArticleContent(full_url)
                            headlineDict = createNewsJson(categories, headline, full_url, content, images)
                            hashKey = str(getHashValue(headlineDict["news"]))
                            toiNews[hashKey] = headlineDict
                #Scraping World News Section
                wndiv = innerDiv.find_all('div', {"id":"c_headlines_wdt_3"})[0]
                for innerDivItem in wndiv:
                    #Scraping News Heading
                    mainCategory = innerDivItem.find("h2").text
                    for item in wndiv.find_all(['h4', 'ul'], {"class":["heading2", "news_card"]}):
                        #Scraping Subheadings
                        if("heading2" in ' '.join(item['class'])):
                            categories = mainCategory+","+item.text
                        else:
                            #Scraping News Lines
                            for listitem in item.find_all('li'):
                                headline = listitem.text.strip()
                                link = listitem.find('a')['href']
                                full_url = base_url + link
                                content, images = scrapeArticleContent(full_url)
                                headlineDict = createNewsJson(categories, headline, full_url, content, images)
                                hashKey = str(getHashValue(headlineDict["news"]))
                                toiNews[hashKey] = headlineDict
                #Scraping Bottom 3 columned news
                col3ndiv = innerDiv.find_all('div', {"id":"c_0201010104"})[0]
                #Scraping News Type
                for innerDivItems in col3ndiv:
                    mainCategory = innerDivItems.h2.text
                    for item in innerDivItems.find_all(['h4','ul']):
                        # Scraping Subheading(Sports Type)
                        if("heading2" in " ".join(item['class'])):
                            categories = mainCategory+","+item.text
                        else:
                            for listitem in item.find_all('li'):
                                headline = listitem.text.strip()
                                link = listitem.find('a')['href']
                                full_url = base_url + link
                                content, images = scrapeArticleContent(full_url)
                                headlineDict = createNewsJson(categories, headline, full_url, content, images)
                                hashKey = str(getHashValue(headlineDict["news"]))
                                toiNews[hashKey] = headlineDict
    with open('news.json', 'w') as f:
        json.dump(toiNews,f)
    return toiNews

def timesOfIndia(inputType, scrapingPath):
    if(inputType == "URL"):
        url = scrapingPath
        pageResponse = requests.get(url)
        data = pageResponse.content
        soup = BeautifulSoup(data,"html.parser")
    else:
        filePath = scrapingPath
        with open(filePath, "r", encoding="utf-8") as fp:
            soup = BeautifulSoup(fp.read(), "html.parser")
        toiHeadlines = scrapeToiHeadlines(soup)
        return toiHeadlines
    
    toiHeadlines = scrapeToiHeadlines(soup)
    return toiHeadlines, pageResponse

def dumpHtmlToFile(htmlreponse, destinationPath):

    try:
        with open(destinationPath, 'wb') as file:
            file.write(htmlreponse.content)
        print(f"HTML page downloaded and saved to {destinationPath}")
        logger.info(f"HTML page downloaded and saved to {destinationPath}")
    except:
        print(f"Failed to dump html to given destination path")
        logger.error("Failed to dump html to given destination path")

This is a project of mine . For this Project write a description just in the format I pasted below

YouTube Video Summarization (Quick Clipper)
Tech Stack: Django, React, Python (Speech-to-Text), Google Search API
• Developed a system to summarize YouTube videos, extracting key points and generating concise
summaries.
• Implemented a chatbot feature to answer user queries and provide video-based insights in real
time.
• Designed APIs to handle video input, process audio-to-text conversion, and return structured
data to the frontend.